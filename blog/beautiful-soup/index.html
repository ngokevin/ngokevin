<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhml"
      xmlns:fb="http://ogp.me/ns/fb#"
      xml:lang="en" lang="en">

  <head>
    <title>ngokevin | Beautiful Soup is Beautiful</title>
    <link rel="stylesheet" type="text/css" href="/css/bundle.css" />
    <link href='http://fonts.googleapis.com/css?family=Ubuntu:400,500,700' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/x-icon" href="/img/favicon.ico?v=5"/>
    <meta name="viewport" content="width=device-width">
    
  <link rel="stylesheet" type="text/css" href="/css/page.css" />
  
    <meta property="og:image" content="http://www.crummy.com/software/BeautifulSoup/10.1.jpg"/>
  
  <meta property="og:description" content="Earlier, I wrote a post about having to populate a table with contact information for network administrators for secure VLANs. I had to do some housekeeping on our crusty database schema using SQLAlchemy to accomodate a couple of new tables. Now, it was time to populate the new firewall contact information table. A lot of the contact information could be found on an intranet site in a nice, neat, and most importantly scrapable table. Previously, I had been using either flat regex or lxml to scrape web pages, but now it was time to play it smart and have a slurp of some Beautiful Soup, a Python HTML parser for screen-scraping. I had heard about Beautiful Soup before, but I had never gotten around to actually playing with it. I was trying out lxml but was put off by its confusing documentation and focus on XML rather than HTML. Boy am I glad that I tried Beautiful Soup. It makes parsing HTML as easy as navigating through the DOM in Javascript in terms of their similar APIs. Obviously, I first had to make a request to the intranet site, which required authentication. Then I had to throw that HTML response into the stew so I could serve some delicious soup: import urllib2 import base64 opener = urllib2.build_opener() request = urllib2.Request(&#34;http://intranet.net.oregonstate.edu/private/firewall-contexts.php&#34;) # authenticate base64string = base64.encodestring(&#39;username:password&#39;)[:-1] request.add_header(&#34;Authorization&#34;, &#34;Basic %s&#34; % base64string) # request and convert to document tree response = opener.open(request) html = response.read() soup = BeautifulSoup(html) Here&#39;s where it gets even easier. I simply needed to extract information from a table. So I grabbed the table, grabbed the rows from the table, and iterated through the rows. What was funny was that this old website had two HTML elements. # get rows from content table content = soup.findAll(&#39;html&#39;)[1] # page has two html elements table = content.table rows = table.findAll(&#39;tr&#39;)[1:] # ignore table header row for row in rows: firewall_contact = {} tds = row.findAll(&#39;td&#39;) # parse firewall context name context = tds[0].findAll(text=True)[0] # parse description description = tds[1].findAll(text=True)[0] # parse administrators try: administrators = tds[2].findAll(text=True)[0].split(&#39;,&#39;) except IndexError: pass What was also funny was that some of the names in the table weren&#39;t their given names. I needed the exact name so I could query for their contact information in LDAP so I wrote a silly hard-coded function that translated names like Andy to Andrew. # lookup adminstrator contact information from ldap info = None for administrator in administrators: name = full(administrator.strip()).split(&#39; &#39;) if len(name) == 2: info = ldap_search(first_name=name[0], last_name=name[1]) if not info: info = ldap_search(first_name=full(name[0]), last_name=name[1]) if not info: info = ldap_search(first_name=full(name[0], alt=True), last_name=name[1]) if not info: continue break else: continue Then I just parsed the rest, threw them into a list of dictionaries, and returned them for SQLAlchemy to have its way with them dictionaries. Nothing like Alchemizing some Beautiful Soup in a stirring cauldron on a cold evening. # parse vlan ids vlans = [] vlan_texts = tds[3].findAll(text=True) for vlan_text in vlan_texts: try: matches = vlan_regex.findall(vlan_text)[0] for match in matches: if match: vlans.append(match) except IndexError: pass # add firewall contact to list of firewall contacts for vlan in vlans: firewall_contact = {} firewall_contact[&#39;context&#39;] = context firewall_contact[&#39;description&#39;] = description firewall_contact[&#39;vlan_id&#39;] = vlan get_ldap_info(info, firewall_contact) if not &#39;name&#39; in firewall_contact: firewall_contact[&#39;name&#39;] = administrators[0] firewall_contacts.append(firewall_contact) return firewall_contacts If I ever have to screen-scrape flat HTML pages again, Beautiful Soup will be my go-to parser."/>

  </head>

  <body>
    <div id="wrap" class="c">
      <header>
        <div class="logos">
          <h1 id="logo">
            <a href='/'>ngo<strong>kevin</strong></a>
            <span class="social">
              <a href="mailto:me@ngokevin.com"><i class="fa fa-envelope"></i></a>
              <a href="http://twitter.com/andgokevin"><i class="fa fa-twitter-square"></i></a>
              <a href="http://github.com/ngokevin"><i class="fa fa-github-square"></i></a>
              <a type="application/rss+xml" href="/rss"><i class="fa fa-rss-square"></i></a>
            </span>
          </h1>
          <span id="sublogo">virtual reality developer at mozilla</span>
          <ul class="blog-tags">
            
              <li>
                <a href="/blog/tags/code/"
                   >
                  Code</a>
              </li>
            
              <li>
                <a href="/blog/tags/life/"
                   >
                  Life</a>
              </li>
            
              <li>
                <a href="/blog/tags/photography/"
                   >
                  Shoot</a>
              </li>
            
              <li>
                <a href="/blog/tags/poker/"
                   >
                  Poker</a>
              </li>
            
          </ul>
        </div>

        <nav>
          
            <a href="/about/">
              <span>
                About</span>
            </a>
          
            <a href="/">
              <span>
                Write</span>
            </a>
          
            <a href="/photography/">
              <span>
                Shoot</span>
            </a>
          
        </nav>
      </header>

      <div class="main">
        <div class="header-group">
  <div class="blog">
    <h1>Beautiful Soup is Beautiful</h1>

    <p class="metadata">
      
        Thursday December 15, 2011
      
    </p>
  </div>

  <div id="top-share" class="addthis_toolbox addthis_default_style">
  <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
  <a class="addthis_button_tweet"></a>
</div>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-513af3cd5e69b4ae"></script>
</div>

        <div class="base-image">
          
            <div class="page_pic right">
              <img src="http://www.crummy.com/software/BeautifulSoup/10.1.jpg">
              
            </div>
          
        </div>

        
  <div class="page c">
    <p>Earlier, I wrote a <a href="http://ngokevin.com/blog/20111215-sqlalchemy/">post</a> about
having to populate a table with contact information for network administrators
for secure VLANs. I had to do some housekeeping on our crusty database schema
using SQLAlchemy to accomodate a couple of new tables. Now, it was time to
populate the new firewall contact information table. A lot of the contact
information could be found on an intranet site in a nice, neat, and most
importantly scrapable table. Previously, I had been using either flat regex or
lxml to scrape web pages, but now it was time to play it smart and have a slurp
of some <a href="www.crummy.com/software/BeautifulSoup/">Beautiful Soup</a>, a Python HTML
parser for screen-scraping.</p>
<p>I had heard about Beautiful Soup before, but I had never gotten around to
actually playing with it. I was trying out lxml but was put off by its
confusing documentation and focus on XML rather than HTML. Boy am I glad that I
tried Beautiful Soup. It makes parsing HTML as easy as navigating through the
DOM in Javascript in terms of their similar APIs.</p>
<p>Obviously, I first had to make a request to the intranet site, which required
authentication. Then I had to throw that HTML response into the stew so I could
serve some delicious soup:</p>
<div class="highlight"><pre><span class="s s-Atom">import</span> <span class="s s-Atom">urllib2</span>
<span class="s s-Atom">import</span> <span class="s s-Atom">base64</span>

<span class="s s-Atom">opener</span> <span class="o">=</span> <span class="s s-Atom">urllib2</span><span class="p">.</span><span class="nf">build_opener</span><span class="p">()</span>
<span class="s s-Atom">request</span> <span class="o">=</span> <span class="s s-Atom">urllib2</span><span class="p">.</span><span class="nv">Request</span><span class="p">(</span><span class="s2">&quot;http://intranet.net.oregonstate.edu/private/firewall-contexts.php&quot;</span><span class="p">)</span>

<span class="c1"># authenticate</span>
<span class="s s-Atom">base64string</span> <span class="o">=</span> <span class="s s-Atom">base64</span><span class="p">.</span><span class="nf">encodestring</span><span class="p">(</span><span class="s s-Atom">&#39;username:password&#39;</span><span class="p">)[:-</span><span class="mi">1</span><span class="p">]</span>
<span class="s s-Atom">request</span><span class="p">.</span><span class="nf">add_header</span><span class="p">(</span><span class="s2">&quot;Authorization&quot;</span><span class="p">,</span> <span class="s2">&quot;Basic %s&quot;</span> <span class="c1">% base64string)</span>

<span class="c1"># request and convert to document tree</span>
<span class="s s-Atom">response</span> <span class="o">=</span> <span class="s s-Atom">opener</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="s s-Atom">request</span><span class="p">)</span>
<span class="s s-Atom">html</span> <span class="o">=</span> <span class="s s-Atom">response</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>
<span class="s s-Atom">soup</span> <span class="o">=</span> <span class="nv">BeautifulSoup</span><span class="p">(</span><span class="s s-Atom">html</span><span class="p">)</span>
</pre></div>


<p>Here's where it gets even easier. I simply needed to extract information from a
table. So I grabbed the table, grabbed the rows from the table, and iterated
through the rows. What was funny was that this old website had two HTML elements.</p>
<div class="highlight"><pre># get rows from content table
content = soup.findAll(&#39;html&#39;)[1] # page has two html elements
table = content.table
rows = table.findAll(&#39;tr&#39;)[1:] # ignore table header row

for row in rows:
    firewall_contact = {}
    tds = row.findAll(&#39;td&#39;)

    # parse firewall context name
    context = tds[0].findAll(text=True)[0]

    # parse description
    description = tds[1].findAll(text=True)[0]

    # parse administrators
    try:
        administrators = tds[2].findAll(text=True)[0].split(&#39;,&#39;)
    except IndexError:
        pass
</pre></div>


<p>What was also funny was that some of the names in the table weren't their given
names. I needed the exact name so I could query for their contact information
in LDAP so I wrote a silly hard-coded function that translated names like Andy
to Andrew.</p>
<div class="highlight"><pre>    # lookup adminstrator contact information from ldap
    info = None
    for administrator in administrators:
        name = full(administrator.strip()).split(&#39; &#39;)
        if len(name) == 2:
            info = ldap_search(first_name=name[0], last_name=name[1])
            if not info:
                info = ldap_search(first_name=full(name[0]), last_name=name[1])
            if not info:
                info = ldap_search(first_name=full(name[0], alt=True), last_name=name[1])
            if not info:
                continue
            break
        else:
            continue
</pre></div>


<p>Then I just parsed the rest, threw them into a list of dictionaries, and
returned them for SQLAlchemy to have its way with them dictionaries. Nothing
like Alchemizing some Beautiful Soup in a stirring cauldron on a cold evening.</p>
<div class="highlight"><pre># parse vlan ids
    vlans = []
    vlan_texts = tds[3].findAll(text=True)
    for vlan_text in vlan_texts:
        try:
            matches = vlan_regex.findall(vlan_text)[0]
            for match in matches:
                if match:
                    vlans.append(match)
        except IndexError:
            pass

    # add firewall contact to list of firewall contacts
    for vlan in vlans:
        firewall_contact = {}
        firewall_contact[&#39;context&#39;] = context
        firewall_contact[&#39;description&#39;] = description
        firewall_contact[&#39;vlan_id&#39;] = vlan
        get_ldap_info(info, firewall_contact)
        if not &#39;name&#39; in firewall_contact:
            firewall_contact[&#39;name&#39;] = administrators[0]
        firewall_contacts.append(firewall_contact)

return firewall_contacts
</pre></div>


<p>If I ever have to screen-scrape flat HTML pages again, Beautiful Soup will be
my go-to parser.</p>
  </div>

  <p class="end-sect">&sect;</p>
  <div id="bottom-share" class="addthis_toolbox addthis_default_style">
  <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
  <a class="addthis_button_tweet"></a>
</div>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-513af3cd5e69b4ae"></script>
   <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'ngokevin'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
           var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
           dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
           (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>
  <script src="https://cdn.jsdelivr.net/anchorjs/3.0.0/anchor.min.js"></script>
  <script>anchors.add('h2, h3, h4, h5, h6');</script>

      </div>
    </div>

    <div id="footer">
      <a href="mailto:me@ngokevin.com"><i class="fa fa-envelope"></i></a>
      <a href="http://twitter.com/ngokevin_"><i class="fa fa-twitter-square"></i></a>
      <a href="http://github.com/ngokevin"><i class="fa fa-github-square"></i></a>
      <a type="application/rss+xml" href="/rss/index.xml"><i class="fa fa-rss-square"></i></a>
    </div>

     <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-29871909-1']);
  _gaq.push(['_trackPageview']);

  (function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
  </body>
</html>